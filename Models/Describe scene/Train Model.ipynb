{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3a9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b0bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def collection_data(text):\n",
    "    mapping = dict()\n",
    "    for line in text.split('\\n'):            \n",
    "        token = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id,image_desc = token[0],token[1:]\n",
    "        image_id = token[0].split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92018705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(desc):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    for key,desc_list in desc.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            text=desc_list[i]\n",
    "            text=text.split()\n",
    "            text=[word.lower() for word in text]\n",
    "            text=[word.translate(table) for word in text]\n",
    "            text=[word for word in text if len(word) > 2]\n",
    "            text=[word for word in text if word.isalpha()]\n",
    "            desc_list[i] = ' '.join(text)\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3b4cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text(desc,filename):\n",
    "    lines = list()\n",
    "    file = open(filename,'w')\n",
    "    for key,desc_list in desc.items():\n",
    "        for i in desc_list:\n",
    "            string = key + ' ' + i\n",
    "            lines.append(string)\n",
    "            file.write(string + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087942bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_voc(desc):\n",
    "    all_desc = set()\n",
    "    for key in desc.keys():\n",
    "        [all_desc.update(d.split()) for d in desc[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811b94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'flickr30k_images/captions.txt'\n",
    "doc = load_file(filename)\n",
    "description = collection_data(doc)\n",
    "clean_desc = clean_data(description)\n",
    "voc = to_voc(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef99d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31783"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be70a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'clean2.txt'\n",
    "save_text(clean_desc,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0263fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(filename):\n",
    "    doc = load_file(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1 :\n",
    "            continue\n",
    "        identifer = line.split('.')[0]\n",
    "        dataset.append(identifer)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3afa9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_desc(filename,dataset):\n",
    "    doc = load_file(filename)\n",
    "    desc = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        token=line.split()\n",
    "        image_id , image_desc = token[0],token[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in desc:\n",
    "                desc[image_id] = list()\n",
    "            desc_end = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            desc[image_id].append(desc_end)\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2612174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(filename,dataset):\n",
    "    all_features = load(open(filename,'rb'))\n",
    "    features = {k:all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37020cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : 31783\n"
     ]
    }
   ],
   "source": [
    "trainfile = 'flickr30k_images/train.txt'\n",
    "train = load_set(trainfile)\n",
    "print(\"Dataset : {}\".format(len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4bff416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions : 31783\n"
     ]
    }
   ],
   "source": [
    "train_desc = load_desc(file,train)\n",
    "print(\"Descriptions : {}\".format(len(train_desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f313af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 31783\n"
     ]
    }
   ],
   "source": [
    "photo_features = load_features('Image_Caption_Features_Extract2.pkl',train)\n",
    "print(\"Features = {}\".format(len(photo_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af41a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import keras\n",
    "import tensorflow\n",
    "from pickle import load \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model , to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense , Input , LSTM , Embedding , Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import add \n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array \n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d510ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(desc):\n",
    "    all_desc = list()\n",
    "    for key in desc.keys():\n",
    "        [all_desc.append(d) for d in desc[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34e3b34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158915\n"
     ]
    }
   ],
   "source": [
    "all_desc = to_lines(train_desc)\n",
    "print(len(all_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a45e51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(desc):\n",
    "    lines = to_lines(desc)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfb30a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(train_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a477c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('tokenizer2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54408ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(caption.split()) for caption in all_desc)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f3536da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19604\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d03f942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(token, max_length, desc_list, photo):\n",
    "    x1, x2, y = list(), list(), list()\n",
    "    for desc in desc_list:\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq , out_seq = seq[:i] , seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            x1.append(photo)\n",
    "            x2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(x1), array(x2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acf24c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(1280,))\n",
    "    fe1 = Dropout(0.001)(inputs1)\n",
    "    fe2 = Dense(224,activation='relu')(fe1)\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size,224,mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.001)(se1)\n",
    "    se3 = LSTM(224)(se2)\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(224, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    model = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1678603",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = define_model(vocab_size,max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "216c0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66)]         0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 1280)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 66, 224)      4391296     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1280)         0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 66, 224)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 224)          286944      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 224)          402304      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 224)          0           ['dense[0][0]',                  \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 224)          50400       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 19604)        4410900     ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,541,844\n",
      "Trainable params: 9,541,844\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c838047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(desc, photos, token, max_length , number_img):\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for key, desc_list in desc.items():\n",
    "            n += 1\n",
    "            photo = photos[key][0]\n",
    "            in_img , in_seq, out_word = create_seq(token, max_length, desc_list, photo)\n",
    "            if n == number_img:\n",
    "                yield[[in_img, in_seq], out_word]\n",
    "                in_img = list()\n",
    "                in_seq = list()\n",
    "                out_word = list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d87a8054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 214s 337ms/step - loss: 5.3075 - accuracy: 0.1872\n",
      "635/635 [==============================] - 224s 353ms/step - loss: 4.9191 - accuracy: 0.1997\n",
      "635/635 [==============================] - 226s 357ms/step - loss: 4.6222 - accuracy: 0.2106\n",
      "635/635 [==============================] - 226s 356ms/step - loss: 4.3658 - accuracy: 0.2198\n",
      "635/635 [==============================] - 226s 356ms/step - loss: 4.1360 - accuracy: 0.2307\n",
      "635/635 [==============================] - 227s 358ms/step - loss: 3.9105 - accuracy: 0.2421\n",
      "635/635 [==============================] - 228s 359ms/step - loss: 3.6876 - accuracy: 0.2557\n",
      "635/635 [==============================] - 229s 361ms/step - loss: 3.4670 - accuracy: 0.2711\n",
      "635/635 [==============================] - 229s 362ms/step - loss: 3.2403 - accuracy: 0.2928\n",
      "635/635 [==============================] - 232s 365ms/step - loss: 3.0373 - accuracy: 0.3136\n",
      "635/635 [==============================] - 239s 376ms/step - loss: 2.8246 - accuracy: 0.3469\n",
      "635/635 [==============================] - 231s 364ms/step - loss: 2.6304 - accuracy: 0.3759\n",
      "635/635 [==============================] - 232s 365ms/step - loss: 2.4240 - accuracy: 0.4098\n",
      "635/635 [==============================] - 233s 367ms/step - loss: 2.2339 - accuracy: 0.4465\n",
      "635/635 [==============================] - 234s 369ms/step - loss: 2.0588 - accuracy: 0.4820\n",
      "635/635 [==============================] - 234s 368ms/step - loss: 1.8861 - accuracy: 0.5158\n",
      "635/635 [==============================] - 235s 370ms/step - loss: 1.7206 - accuracy: 0.5516\n",
      "635/635 [==============================] - 236s 371ms/step - loss: 1.5923 - accuracy: 0.5816\n",
      "635/635 [==============================] - 236s 372ms/step - loss: 1.4628 - accuracy: 0.6118\n",
      "635/635 [==============================] - 236s 371ms/step - loss: 1.3390 - accuracy: 0.6380\n",
      "635/635 [==============================] - 236s 372ms/step - loss: 1.2146 - accuracy: 0.6698\n",
      "635/635 [==============================] - 244s 385ms/step - loss: 1.1090 - accuracy: 0.6954\n",
      "635/635 [==============================] - 238s 375ms/step - loss: 1.0197 - accuracy: 0.7185\n",
      "635/635 [==============================] - 238s 375ms/step - loss: 0.9471 - accuracy: 0.7356\n",
      "635/635 [==============================] - 238s 375ms/step - loss: 0.8783 - accuracy: 0.7535\n",
      "635/635 [==============================] - 240s 378ms/step - loss: 0.8321 - accuracy: 0.7648\n",
      "635/635 [==============================] - 239s 376ms/step - loss: 0.7893 - accuracy: 0.7762\n",
      "635/635 [==============================] - 257s 405ms/step - loss: 0.7491 - accuracy: 0.7853\n",
      "635/635 [==============================] - 247s 389ms/step - loss: 0.6919 - accuracy: 0.7996\n",
      "635/635 [==============================] - 259s 407ms/step - loss: 0.6524 - accuracy: 0.8107\n",
      "635/635 [==============================] - 268s 422ms/step - loss: 0.6189 - accuracy: 0.8197\n",
      "635/635 [==============================] - 259s 408ms/step - loss: 0.5916 - accuracy: 0.8250\n",
      "635/635 [==============================] - 261s 412ms/step - loss: 0.5740 - accuracy: 0.8315\n",
      "635/635 [==============================] - 262s 413ms/step - loss: 0.5549 - accuracy: 0.8341\n",
      "635/635 [==============================] - 311s 490ms/step - loss: 0.5475 - accuracy: 0.8352\n",
      "635/635 [==============================] - 278s 437ms/step - loss: 0.5493 - accuracy: 0.8360\n",
      "635/635 [==============================] - 269s 423ms/step - loss: 0.5308 - accuracy: 0.8388\n",
      "635/635 [==============================] - 261s 411ms/step - loss: 0.5006 - accuracy: 0.8494\n",
      "635/635 [==============================] - 270s 426ms/step - loss: 0.4806 - accuracy: 0.8530\n",
      "635/635 [==============================] - 273s 429ms/step - loss: 0.4819 - accuracy: 0.8540\n",
      "635/635 [==============================] - 270s 426ms/step - loss: 0.4759 - accuracy: 0.8527\n",
      "635/635 [==============================] - 275s 434ms/step - loss: 0.4780 - accuracy: 0.8518\n",
      "635/635 [==============================] - 272s 428ms/step - loss: 0.4690 - accuracy: 0.8552\n",
      "635/635 [==============================] - 276s 435ms/step - loss: 0.4610 - accuracy: 0.8585\n",
      "635/635 [==============================] - 286s 451ms/step - loss: 0.4577 - accuracy: 0.8561\n",
      "635/635 [==============================] - 284s 448ms/step - loss: 0.4603 - accuracy: 0.8537\n",
      "635/635 [==============================] - 294s 463ms/step - loss: 0.4364 - accuracy: 0.8617\n",
      "635/635 [==============================] - 296s 467ms/step - loss: 0.4274 - accuracy: 0.8647\n",
      "635/635 [==============================] - 297s 469ms/step - loss: 0.4161 - accuracy: 0.8682\n",
      "635/635 [==============================] - 294s 462ms/step - loss: 0.4207 - accuracy: 0.8651\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "number_img = 50\n",
    "batch_size = len(train_desc) // number_img\n",
    "for i in range(epochs):\n",
    "    generator = data_gen(train_desc, photo_features, tokenizer, max_length,number_img)\n",
    "    model2.fit(generator, epochs=1, steps_per_epoch=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aaa8a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('Best_Model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
